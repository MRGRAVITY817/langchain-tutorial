{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\n\\nThere are 8 planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.',\n",
       " 'There are 8 planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "a = llm.predict(\"How many planets are there in the solar system?\")\n",
    "b = chat.predict(\"How many planets are there in the solar system?\")\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='We have a variety of toppings available for our pizzas. Some popular options include classic Margherita with tomato sauce, fresh mozzarella, and basil, or a Quattro Stagioni with artichokes, mushrooms, ham, and olives. We also have options like Prosciutto e Rucola with prosciutto, arugula, and shaved Parmesan, or a Diavola with spicy salami and chili flakes. Feel free to customize your pizza with any toppings you prefer!')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "\tSystemMessage(content=\"You are a professional italian chef.\"),\n",
    "\tAIMessage(content=\"Welcome to the italian restaurant. What would you like to order?\"),\n",
    "\tHumanMessage(content=\"I'd like to order a pizza. What are the toppings?\"),\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The distance between Earth and Mars varies depending on their positions in their respective orbits. On average, the distance between Earth and Mars is about 225 million kilometers (140 million miles). However, this distance can range from about 54.6 million kilometers (33.9 million miles) when the two planets are at their closest approach (opposition) to about 401 million kilometers (249 million miles) when they are at their farthest apart (conjunction).'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between the {planet_a} and the {planet_b}?\",)\n",
    "\n",
    "prompt = template.format(planet_a=\"Earth\", planet_b=\"Mars\")\n",
    "\n",
    "chat.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='For a classic Italian pizza, we typically use a thin crust topped with tomato sauce, mozzarella cheese, and various toppings such as pepperoni, mushrooms, bell peppers, onions, olives, and basil. Is there a specific type of pizza you have in mind or any dietary restrictions I should be aware of?')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "\t(\"system\", \"You are a professional {nationality} chef.\"),\n",
    "\t(\"ai\", \"Welcome to the {nationality} restaurant. What would you like to order?\"),\n",
    "\t(\"human\", \"I'd like to order a {food}. What are the ingredients?\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(nationality=\"italian\", food=\"pizza\")\n",
    "\n",
    "chat.predict_messages(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "\tdef parse(self, text: str) -> str:\n",
    "\t\titems = text.strip().split(\",\")\n",
    "\t\treturn list(map(str.strip, items))\n",
    "\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(\"Hello,how,are,you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'orange',\n",
       " 'purple',\n",
       " 'black',\n",
       " 'white',\n",
       " 'pink',\n",
       " 'brown']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "\t(\"system\", \"\"\"You are a list generating machine. \n",
    "\tEverything you are asked will be answered with a comma separated list of max {max_items}, in lowercase. Do not reply with anything else.\n",
    "\t\"\"\"),\n",
    "\t(\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "\tmax_items=10,\n",
    "\tquestion=\"What are the colors?\"\n",
    ")\n",
    "\n",
    "output = chat.predict_messages(prompt)\n",
    "\n",
    "p.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (3633774006.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    max_items=10,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "\t\"max_items\": 10,\n",
    "\t\"question\": \"What are the colors?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorial-GmViVmTg-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
